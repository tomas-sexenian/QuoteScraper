## ðŸ“ Project Structure  
  
```  
root/
â”œâ”€â”€ docs/
â”‚  
â”œâ”€â”€ outputs/                  # Non existent until the first execution is finished
â”‚   â”œâ”€â”€ client.log            # Log file  
â”‚   â”œâ”€â”€ data.json             # Scraped data  
â”‚   â””â”€â”€ qa_report.txt         # Quality assurance results  
â”‚  
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”‚   â””â”€â”€ models.py         # Data models  
â”‚   â”‚  
â”‚   â”œâ”€â”€ scraper/              # Scraping logic  
â”‚   â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”‚   â”œâ”€â”€ quote_parser.py   # Parses quote content from pages  
â”‚   â”‚   â”œâ”€â”€ scraper_runner.py # Entry point to run the scraper logic  
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py  
â”‚   â”‚       â”œâ”€â”€ auth.py       # Authentication logic  
â”‚   â”‚       â”œâ”€â”€ constants.py  # Constant values used across scraper  
â”‚   â”‚       â”œâ”€â”€ scraper_utils.py # Helper functions for scraping  
â”‚   â”‚       â””â”€â”€ setup_utils.py   # Initialization/setup helpers  
â”‚   â”‚  
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ __init__.py  
â”‚		â”œâ”€â”€ schema.json       # JSON schema for data validation  
â”‚       â””â”€â”€ qa.py             # QA test script
â”‚  
â”œâ”€â”€ run_scraper.py            # Script to initiate scraper from CLI  
â”œâ”€â”€ requirements.txt          # Python dependencies  
â””â”€â”€ README.md                 # Project structure overview and run instructions  
```  
## ðŸƒâ€â™‚ï¸ How to run

> This project was developed using Python 3.11.11 so a similar version is recommended for running it.
> 
> It is also recommended to install dependencies and run the project in a virtual environment. The first time you run pip install <res of the command>, run it as suggested.

Open a terminal and standing on the root folder, run ```pip install --no-cache-dir --force-reinstall -r requirements.txt``` to install the project requirements and then follow with ```python run_scraper.py``` to run the scraper.
That will create the following files in the root/outputs folder:
- `data.json` containing all the scraped quotes, grouped by page.
- `qa_report.txt` containing the results of simple QA validation over the scraped content
- `client.log` file with information on important occurrences during the scraper execution

> If you wish to see real time logging to the terminal while running the scraper, go to `src.scraper.utils.setup_utils.py` and uncomment lines 24, 25 and 26

## ðŸ“‹ Where to look
- Everything related to `Part 1: System Design Document` is present in the docs folder
- Everything related to `Part 2: Web Scraper Implementation` is present in the rest of the folders and root level files
